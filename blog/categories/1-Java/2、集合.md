---
title: 集合容器
date: 2022-07-13
description: Java 集合， 也叫作容器，主要是由两大接口派生而来：一个是 `Collection`接口，主要用于存放单一元素；另一个是 `Map` 接口，主要用于存放键值对。对于`Collection` 接口，下面又有三个主要的子接口：`List`、`Set` 和 `Queue`。
tags:
 - Collection
categories:
 - Java
publish: true
---

## 一、概念

### 1.1、什么是集合？

Java 集合， 也叫作容器，主要是由两大接口派生而来：一个是 `Collection`接口，主要用于存放单一元素；另一个是 `Map` 接口，主要用于存放键值对。对于`Collection` 接口，下面又有三个主要的子接口：`List`、`Set` 和 `Queue`。

![image-20220731231303837](https://oss.zhulinz.top//img/202207312313492.png)

### 1.2、集合的作用？（为什么用集合）

当我们需要保存一组类型相同的数据的时候，我们应该是用一个容器来保存，这个容器就是数组，但使用数组存储对象具有一定的弊端，实际开发中，存储的数据类型是多种多样的，于是就有集合，集合同样也是用来存储多个数据的。

数组的缺点是一旦声明之后，`长度就不可变了`；同时，声明数组时的数据类型也决定了该数组存储的数据的类型；而且，**数组存储的数据是有序的、可重复的，特点单一。** 但是`集合提高了数据存储的灵活性`，**Java 集合不仅可以用来存储不同类型不同数量的对象，还可以保存具有映射关系的数据。**

### 1.3、如何选择集合？

主要根据集合的特点来选用，比如我们需要根据键值获取到元素值时就选用 `Map` 接口下的集合，需要排序时选择 `TreeMap`,不需要排序时就选择 `HashMap`,需要保证线程安全就选用 `ConcurrentHashMap`。

当我们只需要存放元素值时，就选择实现`Collection` 接口的集合，需要保证元素唯一时选择实现 `Set` 接口的集合比如 `TreeSet` 或 `HashSet`，不需要就选择实现 `List` 接口的比如 `ArrayList` 或 `LinkedList`，然后再根据实现这些接口的集合的特点来选用。

## 二、Collection接口

### 2.1、Collection-List

> **1、List接口的特点**

- List集合类中元素`有序`（添加顺序和取出顺序一致），且`可重复`。
- List集合中的每个元素都有其对应的顺序索引，即`支持索引`。
- List容器中的元素都对应一个整数型的序号记载其在容器中的位置。可以根据序号存取容器中的元素。

> **2、List的遍历方式**

- for循环。
- foreach遍历List，但是不能对某一元素进行操作。
- 迭代器Iterator遍历；直接根据List集合的遍历。

> **3、ArrayList**

**特点：**

- 允许`存储所有元素`，包括null，ArrayList可以加入一个或者多个null。
- 是基于`数组`实现数据存储的（具有扩容机制）。
- ArrayList基本等同于Vector，除了不是线程安全的（源码中没有synchronized修饰，执行效率高），在多线程的情况下不建议使用ArrayList。
- 支持随机访问
- ArrayList实现RandomAccess，获得了快速随机访问存储元素的功能，RandomAccess是一个标记接口，没有任何方法；
- ArrayList实现Cloneable，得到了clone()方法，可以实现克隆功能；
- ArrayList实现Serializable，表示可以被序列化，通过序列化去传输，典型的应用就是hessian协议。

**扩容机制**

`ArrayList` 的底层是数组队列，相当于**动态数组**。与 Java 中的数组相比，它的容量能动态增长。在添加大量元素前，应用程序可以使用`ensureCapacity`操作来增加 `ArrayList` 实例的容量。这可以减少递增式再分配的数量。

`ArrayList`继承于 **`AbstractList`** ，实现了 **`List`**, **`RandomAccess`**, **`Cloneable`**, **`java.io.Serializable`** 这些接口

```java
public class ArrayList<E> extends AbstractList<E>
    implements List<E>, RandomAccess, Cloneable, java.io.Serializable{
}
```

- `RandomAccess` 是一个标志接口，表明实现这个接口的 List 集合是支持**快速随机访问**的。在 `ArrayList` 中，我们即可以通过元素的序号快速获取元素对象，这就是快速随机访问。
- `ArrayList` 实现了 **`Cloneable` 接口** ，即覆盖了函数`clone()`，能被克隆。
- `ArrayList` 实现了 `java.io.Serializable`接口，这意味着`ArrayList`支持序列化，能通过序列化去传输。

**排序方法**

- 使用集合的工具类`Collections`对ArrayList进行排序

  ```java
  List<Integer> numbers = new ArrayList<>();
  Collections.addAll(numbers, 1, 3, 2, 6, 4, 8, 7, 9);
  Collections.sort(numbers);
  System.out.println("numbers：" + numbers.toString());
  //numbers：[1, 2, 3, 4, 6, 7, 8, 9]
  ```

- 使用`stream`，将ArrayList中的元素流化实现排序

  ```java
  //比如现在有一个User类型的List集合，要求根据User对象中的age属性对List集合中的User对象进行排序
  List<User> userList = new ArrayList<>();
  Collections.addAll(userList,
                     new User(1, "Jack", 25, "男"),
                     new User(2, "Jason", 24, "男"),
                     new User(3, "Jimmy", 20, "男"),
                     new User(4, "Lucy", 19, "男"),
                     new User(5, "Tom", 21, "男")
                    );
  userList = userList
      .stream()
      .sorted(Comparator.comparing(User::getAge))
      //.sorted(Comparator.comparing(User::getAge).reversed()) 加上reversed()方法就是逆序排序
      .collect(Collectors.toList());
  System.out.println("userList：" + userList.toString());
  //userList：[User{id=4, name='Lucy', age=19, sex='男'}, User{id=3, name='Jimmy', age=20, sex='男'}, User{id=5, name='Tom', age=21, sex='男'}, User{id=2, name='Jason', age=24, sex='男'}, User{id=1, name='Jack', age=25, sex='男'}]
  ```

- 使用`Comparable`排序

- 使用`Comparator`排序

**遍历方式**

- fori遍历循环List

  ```java
  for (int i = 0; i < list.size(); i++) {
      System.out.println(list.get(i));
  }
  ```

- foreach遍历循环List

  ```java
  for (String s : list) {
      System.out.println(s);
  }
  ```

- 迭代器Iterator遍历List

  ```java
  Iterator<String> iterator = list.iterator();
  while (iterator.hasNext()) {
      System.out.println(iterator.next());
  }
  ```

**Fail-Fast机制**

fail-fast：快速失败系统，通常设计用于停止有缺陷的过程，这是一种理念，在进行系统设计时优先考虑异常情况，一旦发生异常，直接停止并上报。

ArrayList也采用了快速失败的机制，内部有一个成员变量modeCount。在面对并发的修改时，判断modeCount的变量，迭代器很快就会完全失败，而不是冒着在将来某个不确定时间发生任意不确定行为的风险。

**[ArrayList源码阅读](/8-源码阅读/1、ArrayList源码阅读)**

> **4、LinkedList**

**特点：**

- 底层通过`链表`来实现，随着元素的增加不断向链表的后端增加节点。
- 是一个`双向链表`，每一个节点都拥有`指向前后节点的引用`。相比ArrayList来说，LinkedList的`随机访问效率更低`。
- 可以添加任意元素（可以重复），包括null。
- LinkedList实现Deque，Deque 是一个双向队列，也就是既可以先入先出，又可以先入后出，说简单点就是既可以在头部添加元素，也可以在尾部添加元素；
- LinkedList实现Cloneable，得到了clone()方法，可以实现克隆功能；
- LinkedList实现Serializable，表示可以被序列化，通过序列化去传输，典型的应用就是hessian协议。

> **5、CopyOnWriteArrayList**

**什么是CopyOnWrite容器？**

CopyOnWrite容器即`写时复制容器`。指当我们往一个容器添加元素的时候，不是直接往当前容器添加，而是先将当前容器进行Copy，复制出一个新的容器，然后`向新的容器里添加元素`，添加完元素后再将`原容器的引用指向新的容器`。好处是在对CopyOnWrite容器进行并发的读，而不需要加锁。

**特点：**

- 最适用于具有以下特征的应用程序：List大小通常保持很小，只读操作远多于可变操作，需要在遍历期间防止线程间的冲突
- 是`线程安全`的。
- 通常需要复制整个基础数组，因此可变操作（add()、set()和remove()等等）的开销很大。
- 迭代器支持 hashNext()，next()等不可变操作，但不支持可变 remove()等操作。
- 使用迭代器进行遍历的速度很快，并且不会与其他线程发送冲突。在构造迭代器时，迭代器依赖于不变的数组快照。

**CopyOnWriteArrayList的实现原理**

- 包含了成员Lock，每一个CopyOnWriteArrayList都和一个监视器锁lock绑定，通过lock，实现了对CopyOnWriteArrayList的互斥访问。
- **动态数组机制**：内部有一个“volatile数组”(array)来保持数组。在“添加/修改/删除”数据时，都会新建一个数组，并将更新后的数据拷贝到新建的数组中，最后将该数组赋值给“volatile数组”。
- **线程安全机制**：是通过volatile和监视器锁synchronized来实现的。
- 通过“volatile数组”来保存数据的。一个线程读取volatile数组时，总能看到其它线程对该volatile变量最后的写入；就这样，通过volatile提供了“读取到的数据总是最新的”这个机制的 保证。
- 通过监视器锁Synchrnoized来保护数据。在“添加/修改/删除”数据时，会先“获取监视器锁”，再修改完毕之后，先将数据更新到“volatile数组”中，然后再“释放互斥锁”；这样，就达到了保护数据的目的。

```java
/**
     * Appends the specified element to the end of this list.
     *
     * @param e element to be appended to this list
     * @return {@code true} (as specified by {@link Collection#add})
     */
public boolean add(E e) {
    final ReentrantLock lock = this.lock;
    lock.lock();
    try {
        Object[] elements = getArray();
        int len = elements.length;
        Object[] newElements = Arrays.copyOf(elements, len + 1);
        newElements[len] = e;
        setArray(newElements);
        return true;
    } finally {
        lock.unlock();
    }
}

/**
     * Removes the element at the specified position in this list.
     * Shifts any subsequent elements to the left (subtracts one from their
     * indices).  Returns the element that was removed from the list.
     *
     * @throws IndexOutOfBoundsException {@inheritDoc}
     */
public E remove(int index) {
    final ReentrantLock lock = this.lock;
    lock.lock();
    try {
        Object[] elements = getArray();
        int len = elements.length;
        E oldValue = get(elements, index);
        int numMoved = len - index - 1;
        if (numMoved == 0)
            setArray(Arrays.copyOf(elements, len - 1));
        else {
            Object[] newElements = new Object[len - 1];
            System.arraycopy(elements, 0, newElements, 0, index);
            System.arraycopy(elements, index + 1, newElements, index,
                             numMoved);
            setArray(newElements);
        }
        return oldValue;
    } finally {
        lock.unlock();
    }
}

/**
     * Replaces the element at the specified position in this list with the
     * specified element.
     *
     * @throws IndexOutOfBoundsException {@inheritDoc}
     */
public E set(int index, E element) {
    final ReentrantLock lock = this.lock;
    lock.lock();
    try {
        Object[] elements = getArray();
        E oldValue = get(elements, index);

        if (oldValue != element) {
            int len = elements.length;
            Object[] newElements = Arrays.copyOf(elements, len);
            newElements[index] = element;
            setArray(newElements);
        } else {
            // Not quite a no-op; ensures volatile write semantics
            setArray(elements);
        }
        return oldValue;
    } finally {
        lock.unlock();
    }
}
```

读数据的时候不需要加锁，如果读的时候有多个线程正在向CopyOnWriteArrayList添加数据，读还是会读到旧的数据，因为开始读的那一刻就已经确定了读的对象是旧对象。

CopyOnWrite并发容器用于读多写少的并发场景。比如白名单，黑名单等场景。

**缺点：**

- **内存占用问题**：由于CopyOnWrite的写时复制机制，在进行写操作的时候，内存里会同时驻扎两个对象的内存，旧的对象和新写入的对象（复制的时候只是复制容器里的引用，只是在写的时候会创建新对象添加到新容器里，而旧容器的对象还在使用，因此有两份对象内存）。

  针对内存占用问题，可以通过压缩容器中的元素的方法来减少大对象的内存消耗。比如，元素全是10进制的数字，可以考虑把它压缩成36进制或64进制。也可以使用其他的并发容器（ConcurrentHashMap）。

- **数据一致性问题**：CopyOnWrite容器`只能保证数据的最终一致性`，`不能保证数据的实时一致性`。（**当执行add或remove操作没完成时，get获取的仍然是旧数组的元素**）。

**CopyOnWriteArrayList读取时不加锁，只是写入、删除、修改时加锁**，所以一个线程X读取的时候另一个线程Y可能执行remove操作。remove操作首先要获取独占锁，然后进行写时复制操作，就是复制一份当前的array数组，然后在复制的新数组里面删除线程X通过get访问的元素，比如：1。删除完成后让array指向这个新的数组。
在线程x执行get操作的时候并不是直接通过全局array访问数组元素而是通过方法的形参a访问的，**a指向的地址和array指向的地址在调用get方法的那一刻是一样的，都指向了堆内存的数组对象。之后改变array指向的地址并不影响get的访问，因为在调用get方法的那一刻形参a指向的内存地址就已经确定了，不会改变。所以读的仍然是旧数组。**

> **6、Vector**

> **7、各集合之间的区别**

**ArrayList和Vector的区别？**

- `ArrayList` 是 `List` 的主要实现类，底层使用 `Object[ ]`存储，适用于频繁的查找工作，线程不安全 ；
- `Vector` 是 `List` 的古老实现类，底层使用`Object[ ]` 存储，线程安全的。

**Arraylist 与 LinkedList 区别?**

1. **是否保证线程安全：** `ArrayList` 和 `LinkedList` 都是不同步的，也就是不保证线程安全；
2. **底层数据结构：** `Arraylist` 底层使用的是 **`Object` 数组**；`LinkedList` 底层使用的是 **双向链表** 数据结构（JDK1.6 之前为循环链表，JDK1.7 取消了循环。注意双向链表和双向循环链表的区别，下面有介绍到！）
3. 插入和删除是否受元素位置的影响：
   - `ArrayList` 采用数组存储，所以插入和删除元素的时间复杂度受元素位置的影响。 比如：执行`add(E e)`方法的时候， `ArrayList` 会默认在将指定的元素追加到此列表的末尾，这种情况时间复杂度就是 O(1)。但是如果要在指定位置 i 插入和删除元素的话（`add(int index, E element)`）时间复杂度就为 O(n-i)。因为在进行上述操作的时候集合中第 i 和第 i 个元素之后的(n-i)个元素都要执行向后位/向前移一位的操作。
   - `LinkedList` 采用链表存储，所以，如果是在头尾插入或者删除元素不受元素位置的影响（`add(E e)`、`addFirst(E e)`、`addLast(E e)`、`removeFirst()` 、 `removeLast()`），时间复杂度为 O(1)，如果是要在指定位置 `i` 插入和删除元素的话（`add(int index, E element)`，`remove(Object o)`）， 时间复杂度为 O(n) ，因为需要先移动到指定位置再插入。
4. **是否支持快速随机访问：** `LinkedList` 不支持高效的随机元素访问，而 `ArrayList` 支持。快速随机访问就是通过元素的序号快速获取元素对象(对应于`get(int index)`方法)。
5. **内存空间占用：** `ArrayList` 的空 间浪费主要体现在在 list 列表的结尾会预留一定的容量空间，而 LinkedList 的空间花费则体现在它的每一个元素都需要消耗比 ArrayList 更多的空间（因为要存放直接后继和直接前驱以及数据）。

### 2.2、Collection-Set

- `HashSet`(无序，唯一): 基于 `HashMap` 实现的，底层采用 `HashMap` 来保存元素
- `LinkedHashSet`: `LinkedHashSet` 是 `HashSet` 的子类，并且其内部是通过 `LinkedHashMap` 来实现的。有点类似于我们之前说的 `LinkedHashMap` 其内部是基于 `HashMap` 实现一样，不过还是有一点点区别的
- `TreeSet`(有序，唯一): 红黑树(自平衡的排序二叉树)

### 2.3、Collection-Queue

- `PriorityQueue`: `Object[]` 数组来实现二叉堆
- `ArrayQueue`: `Object[]` 数组 + 双指针

## 三、Map接口

### 3.1、HashMap

> **1、什么是HashMap？**

JDK1.8 之前 `HashMap` 由**数组+链表**组成的，数组是 `HashMap` 的主体，链表则是主要为了解决哈希冲突而存在的（“拉链法”解决冲突）。每一个元素是一个Entry结点，包含key、value、hash值，指向下一个元素的next指针四个属性。

JDK1.8 以后在解决哈希冲突时有了较大的变化，当链表长度大于阈值（默认为 8）（将链表转换成红黑树前会判断，如果当前数组的长度小于 64，那么会选择先进行数组扩容，而不是转换为红黑树）时，将**链表转化为红黑树，以减少搜索时间**。

HashMap的底层是由`数组`、`链表`、`红黑树`（当链表中的**节点数量大于等于8**的时候，**同时当前数组中的长度大于等于MIN_TREEIFY_CAPACITY时**，链表会转换为红黑树）实现，往HashMap中添加键值对的步骤：通过`取模运算`得到桶数组的坐标；往`链表或红黑树`中添加节点。

对于取模运算，是通过` hash&（n-1）`实现。hashmap的容量大小是2的幂次方，可以**通过&运算来优化%运算**。如（17%16）等价于（17&（16-1））。HashMap获取桶数组坐标的时候，会先执行hash（key）运算。

> **2、HashMap不保证元素顺序**

HashMap容器`不保证元素顺序`，根据需要可能会对元素`重新哈希`，元素的顺序也会被重新打散，因此不同时间迭代同一个HashMap的顺序可能会不同。并且根据对冲突的处理方式不同，哈希表有两种实现方式，一种`开放地址方式(open addressing)`，另一种是`冲突链表方式(Separate chaining with linked lists)`。

> **3、哈希表**

在将键值对存入数组之前，将key通过哈希算法计算出哈希值，把`哈希值`作为`数组下标`，把该下标对应的位置作为键值对的存储位置，通过该方法建立的数组叫做`哈希表`，存储位置叫做`桶(bucket)`。数组是通过整数下标直接访问元素，哈希表是通过字符串key直接访问元素，也就说哈希表是一种特殊的数组（关联数组），哈希表广泛应用于实现数据的快速查找（在map的key集合中，一旦存储的key的数量特别多，那么在要查找某个key的时候就会变得很麻烦，数组中的key需要挨个比较，哈希的出现，使得这样的比较次数大大减少。）

> **4、核心成员**

```java
//HashMap的初始容量大小；容量必须是2的幂次方倍数
static final int DEFAULT_INITIAL_CAPACITY = 1 << 4; // aka 16  1*2^4=16

//最大容量，如果一个更高的值由任何一个带参数的构造函数隐式指定时使用。必须是 2 <= 1<<30 的幂。
static final int MAXIMUM_CAPACITY = 1 << 30;

//负载因子大小
static final float DEFAULT_LOAD_FACTOR = 0.75f;

//使用树而不是列表的 bin 计数阈值。将元素添加到至少具有这么多节点的 bin 时，bin 将转换为树。
//该值必须大于 2 并且应该至少为 8，以便与树移除中关于在收缩时转换回普通 bin 的假设相吻合。
//从链表变为树的一个阈值
static final int TREEIFY_THRESHOLD = 8;

//在调整大小操作期间 untreeifying（拆分）bin 的 bin 计数阈值。
//应小于 TREEIFY_THRESHOLD，并且最多 6 以在移除时进行收缩检测。
//从树化变为链化的条件。当树化的元素小于6则进行链化
static final int UNTREEIFY_THRESHOLD = 6;

//可对其进行树化的 bin 的最小表容量。 （否则，如果 bin 中有太多节点，则调整表的大小。）
//应至少为 4 TREEIFY_THRESHOLD 以避免调整大小和树化阈值之间的冲突。
static final int MIN_TREEIFY_CAPACITY = 64;
```

> **5、put流程**

![image-20220727203618342](https://oss.zhulinz.top//img/202208011201347.png)

1. 在存放数据时会先通过 hash 方法计算key的hashCode，JDK1.8之前的计算比较复杂，但是效率并不高，JDK1.8将计算出的 hashCode 高低16位进行异或运算，可以保证尽可能多的位数参与运算，并且让结果中的0和1尽量分布均匀，降低哈希冲突的概率，使键值尽可能分散，提高查询效率。`(key == null) ? 0 : (h = key.hashCode()) ^ (h >>> 16);`

2. 计算出hash值后，再将hash值与数组的长度-1进行与操作，这样可以保证索引的范围在数组的范围之内，由于数组长度必须是2的幂次方，-1后必然是011..11这样的形式，进行与运算就可以保证结果的0和1分别更加均匀。

   ```
   if ((tab = table) == null || (n = tab.length) == 0)
       n = (tab = resize()).length;
   ```

3. 根据哈希值计算下标，如果对应小标正好没有存放数据，则直接插入即可否则需要覆盖。`tab[i = (n - 1) & hash])`

4. 判断tab[i]是否为树节点，否则向链表中插入数据，是则向树中插入节点。

5. 如果链表中插入节点的时候，链表长度大于等于8，则需要把链表转换为红黑树。`treeifyBin(tab, hash);`

6. 最后所有元素处理完成后，判断是否超过阈值；`threshold`，超过则扩容。

> **6、扩容机制**

HashMap扩容时会调用 **resize()方法**，且扩容有三种情况：

- 使用`默认构造方法初始化HashMap时`，只设置了负载因子，并没有进行数组的初始化。然后在第一次`put()`的时候会进行扩容，`容量默认为16`。
- 将元素接入链表后，如果链表长度达到8，并且数组长度小于64，也会进行扩容。
- 当哈希表中的条目数超出了`负载因子与当前容量的乘积`时，会调用HashMap的扩容机制。会按照`当前容量的2倍`进行扩容。

**扩容步骤：**

- `扩容：`创建一个新的Entry空数组，长度是原数组的2倍。

- `ReHash：`遍历原Entry数组，把所有的Entry重新hash到新数组中。

  > **为什么要重新hash？**

  扩容后，数组的长度改变了，Hash的规则也随之改变了。Hash的公式---> index=HashCode(key)&(length-1) 。

```java
final Node<K,V>[] resize() {
    Node<K,V>[] oldTab = table;
    int oldCap = (oldTab == null) ? 0 : oldTab.length;
    int oldThr = threshold;
    int newCap, newThr = 0;
    //如果容量不为空，说明已经初始化
    if (oldCap > 0) {
        //如果容量达到了最大容量1<<30则不再扩容
        if (oldCap >= MAXIMUM_CAPACITY) {
            threshold = Integer.MAX_VALUE;
            return oldTab;
        }
        //按旧容量和阈值的2倍计算新容量和阈值
        else if ((newCap = oldCap << 1) < MAXIMUM_CAPACITY &&
                 oldCap >= DEFAULT_INITIAL_CAPACITY)
            newThr = oldThr << 1; // double threshold
    }
    else if (oldThr > 0) // initial capacity was placed in threshold
        // 初始化时，将 threshold 的值赋值给 newCap，
        // HashMap 使用 threshold 变量暂时保存 initialCapacity 参数的值
        newCap = oldThr;
    else {               // zero initial threshold signifies using defaults
        // 调用无参构造方法时，数组桶数组容量为默认容量 1 << 4; aka 16
        // 阀值；是默认容量与负载因子的乘积，0.75
        newCap = DEFAULT_INITIAL_CAPACITY;
        newThr = (int)(DEFAULT_LOAD_FACTOR * DEFAULT_INITIAL_CAPACITY);
    }
    // newThr为0，则使用阀值公式计算容量
    if (newThr == 0) {
        float ft = (float)newCap * loadFactor;
        newThr = (newCap < MAXIMUM_CAPACITY && ft < (float)MAXIMUM_CAPACITY ?
                  (int)ft : Integer.MAX_VALUE);
    }
    threshold = newThr;
    @SuppressWarnings({"rawtypes","unchecked"})
    //初始化数据桶，用于存放key
    Node<K,V>[] newTab = (Node<K,V>[])new Node[newCap];
    table = newTab;
    if (oldTab != null) {
        //如果旧数组桶，oldCap有值，则遍历将键值映射到新数组桶中
        for (int j = 0; j < oldCap; ++j) {
            Node<K,V> e;
            if ((e = oldTab[j]) != null) {
                oldTab[j] = null;
                if (e.next == null)
                    newTab[e.hash & (newCap - 1)] = e;
                else if (e instanceof TreeNode)
                    //split，红黑树的拆分操作。在重新映射时进行操作。
                    ((TreeNode<K,V>)e).split(this, newTab, j, oldCap);
                else { // preserve order
                    Node<K,V> loHead = null, loTail = null;
                    Node<K,V> hiHead = null, hiTail = null;
                    Node<K,V> next;
                    do {
                        next = e.next;
                        if ((e.hash & oldCap) == 0) {
                            if (loTail == null)
                                loHead = e;
                            else
                                loTail.next = e;
                            loTail = e;
                        }
                        else {
                            if (hiTail == null)
                                hiHead = e;
                            else
                                hiTail.next = e;
                            hiTail = e;
                        }
                    } while ((e = next) != null);
                    if (loTail != null) {
                        loTail.next = null;
                        newTab[j] = loHead;
                    }
                    if (hiTail != null) {
                        hiTail.next = null;
                        newTab[j + oldCap] = hiHead;
                    }
                }
            }
        }
    }
    return newTab;
}
```

> **7、HashMap的长度为什么是2的幂次方**

HashMap在确定元素落在数组的位置的时候，会通过` hash & (table.length-1) `来计算hash值。一般是通过取余运算` hash % table.length `，而当HashMap的长度为2的幂次方时，可以满足**hash&(table.length-1) = hash%table.length**。在计算机中，位运算（&）比取模运算（%）更高效。

为了能让 **HashMap 存取高效，尽量较少碰撞，也就是要尽量把数据分配均匀**。Hash 值的范围值-2147483648 到 2147483647，前后加起来大概 40 亿的映射空间，只要哈希函数映射得比较均匀松散，一般应用是很难出现碰撞的。但问题是一个 40 亿长度的数组，内存是放不下的。所以这个散列值是不能直接拿来用的。用之前还要先做对数组的长度取模运算，得到的余数才能用来要存放的位置也就是对应的数组下标。这个数组下标的计算方法是“ `(n - 1) & hash`”。（n 代表数组长度）。

**为什么长度不能为奇数？**

在计算hash的时候，确定落在数组的位置的时候，计算方法是(n - 1) & hash，奇数n-1为偶数，偶数2进制的结尾都是0，经过hash值&运算后末尾都是0，那么0001，0011，0101，1001，1011，0111，1101这几个位置永远都不能存放元素了，空间浪费相当大，更糟的是这种情况中，数组可以使用的位置比数组长度小了很多，这样就会造成空间的浪费而且会增加hash冲突。

> **8、为什么负载因子是0.75？**

如果加载因子比较大，扩容发生的频率比较低，浪费的空间比较小，发生hash冲突的几率比较大。比如，加载因子是1的时候，hashmap长度为128，实际存储元素的数量在64至128之间时间段比较多，这个时间段发生hash冲突比较多，造成数组中其中一条链表比较长，会影响性能。

如果加载因子比较小，扩容发生的频率比较高，浪费的空间比较多，发生hash冲突的几率比较小。比如，加载因子是0.5的时候，hashmap长度为128，当数量达到65的时候会触发扩容，扩容后为原理的256，256里面只存储了65个浪费了。

综合了一下，取了一个平均数0.75作为加载因子。当负载因子为0.75时代入到`泊松分布公式`，计算出来长度为8时，概率=0.00000006，概率很小了，链表长度为8时转红黑树。

> **9、线程安全问题**

1. JDK1.8之前，链表结点的插入使用`头插法`，在多线程操作的时候可能会`产生链表死循环问题`。
2. JDK1.8起，链表结点的插入改为`尾插法`，不会形成环，但是多线程操作时可能会`存在值丢失的问题`。
3. 可以使用`ConcurrentHashMap`，是线程安全的HashMap。JDK1.8之前，数据结构是Segment数组和Entry数组，采用了减小锁粒度的思想使用分段锁来保证线程安全，Segment的数量就是锁的并发度，默认为16，一个Segment包含了一个HashEntry链表，HashEntry用来存储数据，当修改了HashEntry的时候必须先获取对应的Segment锁。
4. JDK1.8起，弃用了Segment分段锁机制，数据结构是Node数组加链表或红黑树。数组以及结点的val和next都使用volatile修饰，当链表长度达到8时Node结点会转为TreeNode，并不是直接转为红黑树而是通过TreeBin对象完成包装。使用CAS机制和Synchronized保证线程安全，因为JDK1.8对Synchronized做了优化，性能已经和显式锁差不多。put和remove方法加锁，get方法不加锁。

> **10、红黑树**

1. 红黑树就是一种`自平衡二叉树`，实现原理和平衡二叉树类似，但性能要优于平衡二叉树。
2. 红黑树的特性：每个结点是`红色或是黑色`、`根节点是黑色`、每个`叶子节点是黑色`、`每个红色结点的两个子结点是黑色`、`从任意结点到每个叶子的路径包含数目相同的黑色结点`。
3. 红黑树的插入过程主要操作有两种：变色，用于调整两个红色结点相邻的情况，适应性质4；旋转，用于调整左右子树黑色结点数目不同的情况，适应性质5。
4. 在HashMap中putTreeVal用于保存树节点，执行二叉树查找，每一次都比较当前结点和待插入结点的大小，如果小就在左子树查找，否则往右子树查找。找到空位后，执行两个方法，balanceInsertion平衡插入，将结点插入红黑树并使之平衡，moveRootToFront重置红黑树的根结点。

> **11、遍历方式**

![image-20220801193114555](https://oss.zhulinz.top//img/202208011932926.png)

1. **使用迭代器（Iterator）EntrySet 的方式进行遍历；**

   ```java
   public class HashMapTest {
       public static void main(String[] args) {
           // 创建并赋值 HashMap
           Map<Integer, String> map = new HashMap();
           map.put(1, "Java");
           map.put(2, "JDK");
           map.put(3, "Spring Framework");
           map.put(4, "MyBatis framework");
           map.put(5, "Java中文社群");
           // 遍历
           Iterator<Map.Entry<Integer, String>> iterator = map.entrySet().iterator();
           while (iterator.hasNext()) {
               Map.Entry<Integer, String> entry = iterator.next();
               System.out.println(entry.getKey());
               System.out.println(entry.getValue());
           }
       }
   }
   ```

2. **使用迭代器（Iterator）KeySet 的方式进行遍历；**

   ```java
   public class HashMapTest {
       public static void main(String[] args) {
           // 创建并赋值 HashMap
           Map<Integer, String> map = new HashMap();
           map.put(1, "Java");
           map.put(2, "JDK");
           map.put(3, "Spring Framework");
           map.put(4, "MyBatis framework");
           map.put(5, "Java中文社群");
           // 遍历
           Iterator<Integer> iterator = map.keySet().iterator();
           while (iterator.hasNext()) {
               Integer key = iterator.next();
               System.out.println(key);
               System.out.println(map.get(key));
           }
       }
   }
   ```

3. **使用 For Each EntrySet 的方式进行遍历；**

   ```java
   public class HashMapTest {
       public static void main(String[] args) {
           // 创建并赋值 HashMap
           Map<Integer, String> map = new HashMap();
           map.put(1, "Java");
           map.put(2, "JDK");
           map.put(3, "Spring Framework");
           map.put(4, "MyBatis framework");
           map.put(5, "Java中文社群");
           // 遍历
           for (Map.Entry<Integer, String> entry : map.entrySet()) {
               System.out.println(entry.getKey());
               System.out.println(entry.getValue());
           }
       }
   }
   ```

4. **使用 For Each KeySet 的方式进行遍历；**

   ```java
   public class HashMapTest {
       public static void main(String[] args) {
           // 创建并赋值 HashMap
           Map<Integer, String> map = new HashMap();
           map.put(1, "Java");
           map.put(2, "JDK");
           map.put(3, "Spring Framework");
           map.put(4, "MyBatis framework");
           map.put(5, "Java中文社群");
           // 遍历
           for (Integer key : map.keySet()) {
               System.out.println(key);
               System.out.println(map.get(key));
           }
       }
   }
   ```

5. **使用 Lambda 表达式的方式进行遍历；**

   ```java
   public class HashMapTest {
       public static void main(String[] args) {
           // 创建并赋值 HashMap
           Map<Integer, String> map = new HashMap();
           map.put(1, "Java");
           map.put(2, "JDK");
           map.put(3, "Spring Framework");
           map.put(4, "MyBatis framework");
           map.put(5, "Java中文社群");
           // 遍历
           map.forEach((key, value) -> {
               System.out.println(key);
               System.out.println(value);
           });
       }
   }
   ```

6. **使用 Streams API 单线程的方式进行遍历；**

   ```java
   public class HashMapTest {
       public static void main(String[] args) {
           // 创建并赋值 HashMap
           Map<Integer, String> map = new HashMap();
           map.put(1, "Java");
           map.put(2, "JDK");
           map.put(3, "Spring Framework");
           map.put(4, "MyBatis framework");
           map.put(5, "Java中文社群");
           // 遍历
           map.entrySet().stream().forEach((entry) -> {
               System.out.println(entry.getKey());
               System.out.println(entry.getValue());
           });
       }
   }
   ```

7. **使用 Streams API 多线程的方式进行遍历。**

   ```java
   public class HashMapTest {
       public static void main(String[] args) {
           // 创建并赋值 HashMap
           Map<Integer, String> map = new HashMap();
           map.put(1, "Java");
           map.put(2, "JDK");
           map.put(3, "Spring Framework");
           map.put(4, "MyBatis framework");
           map.put(5, "Java中文社群");
           // 遍历
           map.entrySet().parallelStream().forEach((entry) -> {
               System.out.println(entry.getKey());
               System.out.println(entry.getValue());
           });
       }
   }
   ```

**[HashMap源码阅读](/8-源码阅读/2、HashMap源码阅读)**

### 3.2、LinkedHashMap

> #### 1、概述

 `LinkedHashMap` 继承自 `HashMap`，所以它的底层仍然是`基于拉链式散列结构`即由`数组和链表或红黑树`组成。另外，`LinkedHashMap` 在上面结构的基础上，增加了一条**双向链表**，使得上面的结构可以**保持键值对的插入顺序**。同时通过对链表进行相应的操作，实现了访问顺序相关逻辑。详细可以查看：[《LinkedHashMap 源码详细分析（JDK1.8）》open in new window](https://www.imooc.com/article/22931)

LinkedHashMap类新增三个主要属性：head（list头部节点）、tail（list尾部节点）、accessOrder（list节点顺序是否随get方法的调用而改变）。

LinkedHashMap中存储key-value的内部类Entry<K,V>继承自HashMap.Node<K,V>，并新增了before和after指针。

![image-20220829171357366](https://oss.zhulinz.top//img/202208291714868.png)

> #### 2、构造器

`LinkedHashMap`类一共有5个构造器，涉及`initialCapacity`、`loadFactor`、`accessOrder`三个参数的初始化。

- `LinkedHashMap`调用的是父类`HashMap`的构造器
- 默认`accessOrder = false;`，可通过构造函数进行赋值
- `HashMap.table`长度默认**16**，`HashMap`默认负载因子为**0.75**

> #### 3、LinkedHashMap操作后回调

在`HashMap`中有三个方法是用于`LinkedHashMap`操作后回调，在`HashMap`中的各个方法中都有相应的调用，三个方法都是空实现，都要在`LinkedHashMap`中进行实现

- `afterNodeRemoval`：删除某个节点时，进行的操作：

  - 先将需要删除的节点的前后指针赋值给两个变量
  - 将删除的节点的`before`和`after`置为null
  - 将节点的后继的`before`指向节点的前驱节点
  - 将节点的前驱的`after`指向节点的后继节点

- `afterNodeInsertion`：插入某个节点时的操作：

  - 实现中，判断了`removeEldestEntry(first)`方法的返回值，默认为`false`，也就是说并不会进入if中的逻辑，即不会执行其中的`removeNode`方法

  ```java
  /**
   * 插入一个节点后的操作
   */
  void afterNodeInsertion(boolean evict) { // possibly remove eldest
      LinkedHashMap.Entry<K,V> first;
      // LinkedHashMap中的removeEldestEntry方法永远返回false（方法体见下文）
      if (evict && (first = head) != null && removeEldestEntry(first)) {
          K key = first.key;
          // 这里不会执行
          removeNode(hash(key), key, null, false, true);
      }
  }
  /**
   * removeEldestEntry方法永远返回false
   */
  protected boolean removeEldestEntry(Map.Entry<K,V> eldest) {
      return false;
  }
  ```

- `afterNodeAccess`：访问某个节点时的操作

  - 此方法是否执行依据`accessOrder == true`，当调用`get`方法，会将`key-value`节点移动到链表尾端
  - 当`accessOrder == true`，且节点不在尾端时，执行移动操作
  - ① 先将节点的前后指针赋值给两个变量
  - ② 将节点的前驱的`after`指向节点的后继节点
  - ③ 将节点的后继的`before`指向节点的前驱节点
  - ④ 将节点指向链表的尾端
  - ⑤ 更新链表的新尾端tail

> #### 4、链表实现的原理

- `get`方法：`LinkedHashMap`中重写了`get`方法

  - 调用`HashMap#getNode`方法，若为null，则直接返回null
  - 当`accessOrder == true`时，调用`afterNodeAccess`方法，即将访问节点放到链表尾端

- `getOrDefault`方法：`LinkedHashMap`提供一个`getOrDefault`方法，没找到就返回提供的默认值

  - 同`get`方法一样，只不过提供了一个默认参数值，在`HashMap#getNode`方法为null时返回默认值

- `put`方法：在`LinkedHashMap`中并没有重写`put`相关的方法，直接调用的是`HashMap#put`方法

  - 当节点插入成功时，调用`afterNodeAccess`方法

  - 在`put`方法最后，会调用`afterNodeInsertion`方法

- `remove`方法：在`LinkedHashMap`中并没有重写`remove`相关的方法，直接调用的是`HashMap#remove`方法

  - `remove`方法中仅调用了`removeNode`方法
  - 在`removeNode`方法中，在节点移除成功后，调用了`afterNodeRemoval`方法

> #### 5、LRU基于LinkedHashMap的实现

- LRU，最近最少原则，若保存的数据满了，则将最近最少使用的数据删除
- 通过将`LinkedHashMap#accessOrder`设为`true`时，可满足此特性
- 重写`removeEldestEntry`方法，返回`size() > capacity`来决定是否要执行删除节点的操作，上面提到，在`LinkedHashMap`中此方法默认返回`false`，则不会执行删除的节点
- LRU的逻辑为：
  - 将`accessOrder`置为`true`，则调用`get`方法，会将访问的节点移到链表尾端
  - 根据集合大小是否超过容量，来决定是否要删除头部节点，以此来删除最久未用的节点

```java
class LRUCache extends LinkedHashMap {

    private int capacity;

    public LRUCache(int capacity) {
        //accessOrder为true
        super(capacity, 0.75F, true);
        this.capacity = capacity;
    }

    public int get(int key) {
        return (int)super.getOrDefault(key, -1);
    }

    public void put(int key, int value) {
        super.put(key, value);
    }

    protected boolean removeEldestEntry(Map.Entry eldest) {
        return size() > capacity;
    }
}
```

### 3.3、ConcurrentHashMap

- JDK1.7采用分段锁（为Segment数组加锁ReentrantLock）的方式实现。
- JDK1.8采用`CAS` + `synchronized`，即链头加锁的方式
- `CAS`：`Compare And Swap`，它是一种乐观锁，认为对于同一个数据的并发操作不一定会发生修改，在更新数据的时候，尝试去更新数据，如果失败就不断尝试。

> #### 1、JDK1.7实现原理

- `ConcurrentHashMap` 类所采用的是分段锁的思想，将 `HashMap` 进行切割
- 把 HashMap 中的哈希数组切分成**小数组**，每个小数组有 n 个 `HashEntry` 组成
- **小数组**继承自`ReentrantLock（可重入锁）`，这个小数组名叫`Segment`
- 可以将 `ConcurrentHashMap` 看作一个二级哈希表。在一个总的哈希表下面，有若干个子哈希表。
- 当hash冲突的链表过长时，在查询遍历的时候依然很慢！

**`get`方法**

- 根据key的hash值，找到对应位置的`Segment`对象
- 再通过key的hash值，定位到`Segment`对象中数组的对应位置

 **`put`方法**

- 根据key的hash值，找到对应位置的`Segment`对象
- 获取`ReentrantLock`可重入锁
- 再通过key的hash值，定位到`Segment`对象中数组的对应位置
- 插入或覆盖`HashEntry`对象
- 释放锁

> #### 2、JDK1.8实现原理

- `ConcurrentHashMap`采用了`CAS + synchronized` 来保证并发的安全性
- 节点Node类中的共享变量，使用了`volatile`关键字，保证多线程操作的可见性，JDK1.7一样

 **`get`方法**

- 通过key的hash值定位到数组位置
- 判断node节点的首个元素是否为目标元素，是则返回
- 若是红黑树结构，则从树中查找
- 若是链表结构，则遍历向后查找

**`remove`方法**

- ① 循环遍历数组，接着校验参数；
- ② 判断是否有别的线程正在扩容，如果是一起扩容；
- ③ 用 synchronized 同步锁，保证并发时元素移除安全；
- ④ 因为 `check= -1`，所以不会进行扩容操作，利用CAS操作修改baseCount值。

**`put`方法**

- key和value都不能为null，否则抛出空指针异常
- 判断节点数组容量是否为null，为null则初始化数组`initTable`
- 根据hash是否查找到节点，若不存在，则通过CAS方式插入：若已有在插入的元素则进入下一次循环；若插入成功，则跳出循环，方法结束
- 判断是否有其他线程在扩容，即`f.hash == MOVE`（MOVE为-1）时，f为`ForwardingNode`节点；则执行`helpTransfer`，一起扩容
- 上面条件都不满足时，则在`synchronized`中进行插入赋值，即把新的`Node`节点按链表或红黑树的方式插入到合适的位置
- 当在`synchronized`中插入成功后，判断是否要进行链表向红黑树的转化
- 在最后，当成功插入元素，元素个数加1，判断是否要扩容

**`initTable`初始化数组**

- 当数组为null或容量为0时，进行初始化操作

- 先判断`sizeCtl<0`，即是否正在初始化或扩容，则`Thread.yield();`让出CPU

- 否则无初始化或扩容操作时，通过CAS锁控制只有一个线程进行初始化tab数组，

  ```
  sizeCtl
  ```

  更新为-1成功

  - 先检查table是否为null或容量为0，避免ABA问题
  - 新建Node数组，根据`sc>0`来设定容量，默认为16；并赋值为table
  - 设置`sc`大小为数组长度的0.75倍
  - 最后，将`sc`赋值给`sizeCtl`，设定扩容门槛
  - 结束

**`helpTransfer` 协助扩容**

- 当满足一定条件（传入tab数组不为null，且tab首个元素f为

  ```
  ForwardingNode
  ```

  类型，且f的

  ```
  nextTab
  ```

  不为null）时，才会进行协助扩容

  - 说明当前tab已迁完，才会去协助迁移其他tab元素
  - 在扩容时会把旧tab的首个元素置为`ForwardingNode`，并让其`nextTab`指向新tab数组

- 当`sizeCtl<0`，说明正在扩容，才会协助扩容

- 扩容线程加1后，让当前线程协助迁移元素：`transfer(tab, nextTab);`

**`addCount` 扩容判断**

- ① 利用CAS将方法更新baseCount的值
- ② 检查是否需要扩容，默认check = 1，需要检查；
- ③ 如果满足扩容条件，判断当前是否正在扩容，如果是正在扩容就一起扩容；
- ④ 如果不在扩容，将sizeCtl更新为负数，并进行扩容处理。

> #### 3、几种场景下使用ConcurrentHashMap

**不存在（null）则插入**

当多线程下同时调用如下`unsafeUpdate`方法，可能会导致在**get()之后if之前**有其他线程进行put操作了，则当前线程的put会覆盖之前线程的put值；可使用putIfAbsent()替换：

```java
private static final Map<Integer, Integer> map = new ConcurrentHashMap<>();

public void unsafeUpdate(Integer key, Integer value) {
    Integer oldValue = map.get(key);
    if (oldValue == null) {
        map.put(key, value);
    }
}

// 修改如下：
public void safeUpdate(Integer key, Integer value) {
    map.putIfAbsent(key, value);
}
```

**特定值则修改**

当多线程下同时调用如下`unsafeUpdate`方法，，可以使用`replace(K key, V oldValue, V newValue)`方法，但传入的`newValue`若为null，则表示删除此元素

```java
private static final Map<Integer, Integer> map = new ConcurrentHashMap<>();

public void unsafeUpdate(Integer key, Integer value) {
    Integer oldValue = map.get(key);
    if (oldValue == 1) {
        map.put(key, value);
    }
}

// 修改如下：
public void safeUpdate(Integer key, Integer value) {
    map.replace(key, 1, value);
}
```

**处理后插入**

当`get`值之后，进行相关逻辑的处理后，再进行`put`操作的情况下，`ConcurrentHashMap`提供的方法则不一定能保证线程安全了，则可以通过加同步锁`synchronized`的方法来进行处理，如:

```java
public void unsafeUpdate(Integer key, Integer value) {
    Integer oldValue = map.get(key);
    if (oldValue == 1) {
        System.out.println(System.currentTimeMillis());
        /**
         * 其它业务操作
         */
        System.out.println(System.currentTimeMillis());
      
        map.put(key, value);
    }
}

// 修改后：
public void safeUpdate(Integer key, Integer value) {
    synchronized (map) {
        Integer oldValue = map.get(key);
        if (oldValue == null) {
            System.out.println(System.currentTimeMillis());
            /**
             * 其它业务操作
             */
            System.out.println(System.currentTimeMillis());

            map.put(key, value);
        }
    }
}
```

但此时使用`ConcurrentHashMap`意义不大，可以换成普通的`HashMap`进行同步安全的操作

> #### 4、几种实现线程安全集合的方式

**`Hashtable`线程安全类**

- `Hashtable` 是一个线程安全的类
- `Hashtable` 几乎所有的添加、删除、查询方法都加了`synchronized`同步锁！
- 相当于给整个哈希表加了一把大锁，多线程访问时候，只要有一个线程访问或操作该对象，那其他线程只能阻塞等待需要的锁被释放，在竞争激烈的多线程场景中性能就会非常差，**所以 Hashtable 不推荐使用！**

**`Collections.synchronizedMap`方法**

- `Collections.synchronizedMap` 里面使用对象锁来保证多线程场景下，操作安全，本质也是对 HashMap 进行全表锁
- **使用`Collections.synchronizedMap`方法，在竞争激烈的多线程环境下性能依然也非常差，所以不推荐使用！**

### 3.4、TreeMap

红黑树（自平衡的排序二叉树

### 3.5、HashTable

数组+链表组成的，数组是 `Hashtable` 的主体，链表则是主要为了解决哈希冲突而存在的

### 3.6、常见题目？

> **1、HashMap 和 Hashtable 的区别**

1. **线程是否安全：** `HashMap` 是非线程安全的，`Hashtable` 是线程安全的,因为 `Hashtable` 内部的方法基本都经过`synchronized` 修饰。（如果你要保证线程安全的话就使用 `ConcurrentHashMap` 吧！）；
2. **效率：** 因为线程安全的问题，`HashMap` 要比 `Hashtable` 效率高一点。另外，`Hashtable` 基本被淘汰，不要在代码中使用它；
3. **对 Null key 和 Null value 的支持：** `HashMap` 可以存储 null 的 key 和 value，但 null 作为键只能有一个，null 作为值可以有多个；Hashtable 不允许有 null 键和 null 值，否则会抛出 `NullPointerException`。
4. **初始容量大小和每次扩充容量大小的不同 ：** ① 创建时如果不指定容量初始值，`Hashtable` 默认的初始大小为 11，之后每次扩充，容量变为原来的 2n+1。`HashMap` 默认的初始化大小为 16。之后每次扩充，容量变为原来的 2 倍。② 创建时如果给定了容量初始值，那么 Hashtable 会直接使用你给定的大小，而 `HashMap` 会将其扩充为 2 的幂次方大小（`HashMap` 中的`tableSizeFor()`方法保证）。也就是说 `HashMap` 总是使用 2 的幂作为哈希表的大小。
5. **底层数据结构：** JDK1.8 以后的 `HashMap` 在解决哈希冲突时有了较大的变化，当链表长度大于阈值（默认为 8）（将链表转换成红黑树前会判断，如果当前数组的长度小于 64，那么会选择先进行数组扩容，而不是转换为红黑树）时，将链表转化为红黑树，以减少搜索时间。Hashtable 没有这样的机制。

> **2、HashMap 和 HashSet 区别**

`HashSet` 底层就是基于 `HashMap` 实现的。（`HashSet` 的源码非常非常少，因为除了 `clone()`、`writeObject()`、`readObject()`是 `HashSet` 自己不得不实现之外，其他方法都是直接调用 `HashMap` 中的方法。

|               `HashMap`                |                          `HashSet`                           |
| :------------------------------------: | :----------------------------------------------------------: |
|           实现了 `Map` 接口            |                       实现 `Set` 接口                        |
|               存储键值对               |                          仅存储对象                          |
|     调用 `put()`向 map 中添加元素      |             调用 `add()`方法向 `Set` 中添加元素              |
| `HashMap` 使用键（Key）计算 `hashcode` | `HashSet` 使用成员对象来计算 `hashcode` 值，对于两个对象来说 `hashcode` 可能相同，所以`equals()`方法用来判断对象的相等性 |

> **3、HashMap 和 TreeMap 区别**

`TreeMap` 和`HashMap` 都继承自`AbstractMap` ，但是需要注意的是`TreeMap`它还实现了`NavigableMap`接口和`SortedMap` 接口。

> **4、HashMap 的底层实现**

JDK1.8 之前 `HashMap` 底层是 **数组和链表** 结合在一起使用也就是 **链表散列**。**HashMap 通过 key 的 hashCode 经过扰动函数处理过后得到 hash 值，然后通过 (n - 1) & hash 判断当前元素存放的位置（这里的 n 指的是数组的长度），如果当前位置存在元素的话，就判断该元素与要存入的元素的 hash 值以及 key 是否相同，如果相同的话，直接覆盖，不相同就通过拉链法解决冲突。**

**所谓扰动函数指的就是 HashMap 的 hash 方法。使用 hash 方法也就是扰动函数是为了防止一些实现比较差的 hashCode() 方法 换句话说使用扰动函数之后可以减少碰撞。**

**JDK 1.8 HashMap 的 hash 方法源码:**

```java
static final int hash(Object key) {
      int h;
      // key.hashCode()：返回散列值也就是hashcode
      // ^ ：按位异或
      // >>>:无符号右移，忽略符号位，空位都以0补齐
      return (key == null) ? 0 : (h = key.hashCode()) ^ (h >>> 16);
}
```

对比一下 JDK1.7 的 HashMap 的 hash 方法源码.

```java
static int hash(int h) {
    // This function ensures that hashCodes that differ only by
    // constant multiples at each bit position have a bounded
    // number of collisions (approximately 8 at default load factor).

    h ^= (h >>> 20) ^ (h >>> 12);
    return h ^ (h >>> 7) ^ (h >>> 4);
}
```

JDK1.8 之后

相比于之前的版本， JDK1.8 之后在解决哈希冲突时有了较大的变化，当链表长度大于阈值（默认为 8）（将链表转换成红黑树前会判断，如果当前数组的长度小于 64，那么会选择先进行数组扩容，而不是转换为红黑树）时，将链表转化为红黑树，以减少搜索时间。

<img src="https://oss.zhulinz.top//img/jdk1.8之后的内部结构-HashMap.ae1eae18.png"/>

## 参考文章

- [集合八股文](/7-八股文/3、Java集合面试题)